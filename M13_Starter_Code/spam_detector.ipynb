{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Data\n",
    "\n",
    "The data is located at [https://static.bc-edx.com/ai/ail-v-1-0/m13/challenge/spam-data.csv](https://static.bc-edx.com/ai/ail-v-1-0/m13/challenge/spam-data.csv)\n",
    "\n",
    "Dataset Source: [UCI Machine Learning Library](https://archive.ics.uci.edu/dataset/94/spambase)\n",
    "\n",
    "Import the data using Pandas. Display the resulting DataFrame to confirm the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"https://static.bc-edx.com/ai/ail-v-1-0/m13/challenge/spam-data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Model Performance\n",
    "\n",
    "You will be creating and comparing two models on this data: a Logistic Regression, and a Random Forests Classifier. Before you create, fit, and score the models, make a prediction as to which model you think will perform better. You do not need to be correct! \n",
    "\n",
    "Write down your prediction in the designated cells in your Jupyter Notebook, and provide justification for your educated guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace the text in this markdown cell with your predictions, and be sure to provide justification for your guess.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a random guess that the logistic regression model will be better. That is my prediction. Educated guess will be hard at this point as I am not sure what to say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
      "0            0.00               0.64           0.64           0.0   \n",
      "1            0.21               0.28           0.50           0.0   \n",
      "2            0.06               0.00           0.71           0.0   \n",
      "3            0.00               0.00           0.00           0.0   \n",
      "4            0.00               0.00           0.00           0.0   \n",
      "\n",
      "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
      "0           0.32            0.00              0.00                0.00   \n",
      "1           0.14            0.28              0.21                0.07   \n",
      "2           1.23            0.19              0.19                0.12   \n",
      "3           0.63            0.00              0.31                0.63   \n",
      "4           0.63            0.00              0.31                0.63   \n",
      "\n",
      "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
      "0             0.00            0.00  ...                   0.0         0.00   \n",
      "1             0.00            0.94  ...                   0.0         0.00   \n",
      "2             0.64            0.25  ...                   0.0         0.01   \n",
      "3             0.31            0.63  ...                   0.0         0.00   \n",
      "4             0.31            0.63  ...                   0.0         0.00   \n",
      "\n",
      "   char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
      "0        0.000          0.0        0.778        0.000        0.000   \n",
      "1        0.132          0.0        0.372        0.180        0.048   \n",
      "2        0.143          0.0        0.276        0.184        0.010   \n",
      "3        0.137          0.0        0.137        0.000        0.000   \n",
      "4        0.135          0.0        0.135        0.000        0.000   \n",
      "\n",
      "   capital_run_length_average  capital_run_length_longest  \\\n",
      "0                       3.756                          61   \n",
      "1                       5.114                         101   \n",
      "2                       9.821                         485   \n",
      "3                       3.537                          40   \n",
      "4                       3.537                          40   \n",
      "\n",
      "   capital_run_length_total  \n",
      "0                       278  \n",
      "1                      1028  \n",
      "2                      2259  \n",
      "3                       191  \n",
      "4                       191  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: spam, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create the labels set `y` and features DataFrame `X`\n",
    "# Create the labels set `y`\n",
    "y = data['spam']\n",
    "\n",
    "# Create the features DataFrame `X`\n",
    "X = data.drop(columns=['spam'])\n",
    "\n",
    "# Display the first few rows of X and y to confirm\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n",
      "0    2788\n",
      "1    1813\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the balance of the labels variable (`y`) by using the `value_counts` function.\n",
    "# Check the balance of the labels\n",
    "label_balance = y.value_counts()\n",
    "\n",
    "# Display the balance of the labels\n",
    "print(label_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3680, 57)\n",
      "X_test shape: (921, 57)\n",
      "y_train shape: (3680,)\n",
      "y_test shape: (921,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets to confirm the split\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.444</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.090</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.846</td>\n",
       "      <td>39</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.037</td>\n",
       "      <td>18</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "4019            0.00                0.0            0.0           0.0   \n",
       "1777            0.00                0.0            0.0           0.0   \n",
       "3574            0.75                0.0            0.0           0.0   \n",
       "2322            0.00                0.0            0.1           0.0   \n",
       "3596            0.00                0.0            0.0           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "4019           0.00             0.0               0.0                 0.0   \n",
       "1777           1.29             0.0               0.0                 0.0   \n",
       "3574           0.00             0.0               0.0                 0.0   \n",
       "2322           0.10             0.0               0.0                 0.0   \n",
       "3596           0.00             0.0               0.0                 0.0   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
       "4019             0.00             0.0  ...                   0.0          0.0   \n",
       "1777             0.00             0.0  ...                   0.0          0.0   \n",
       "3574             0.75             0.0  ...                   0.0          0.0   \n",
       "2322             0.00             0.0  ...                   0.6          0.0   \n",
       "3596             0.00             0.0  ...                   0.0          0.0   \n",
       "\n",
       "      char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "4019        1.176          0.0        0.000          0.0        0.000   \n",
       "1777        0.224          0.0        2.354          0.0        0.000   \n",
       "3574        0.000          0.0        0.000          0.0        0.000   \n",
       "2322        0.096          0.0        0.000          0.0        0.012   \n",
       "3596        0.000          0.0        0.000          0.0        0.000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "4019                       3.444                          11   \n",
       "1777                       2.090                          13   \n",
       "3574                       3.846                          39   \n",
       "2322                       2.037                          18   \n",
       "3596                       1.000                           1   \n",
       "\n",
       "      capital_run_length_total  \n",
       "4019                        31  \n",
       "1777                        69  \n",
       "3574                       100  \n",
       "2322                       913  \n",
       "3596                         5  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `StandardScaler` to scale the features data. Remember that only `X_train` and `X_test` DataFrames should be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the StandardScaler instance\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Standard Scaler with the training data?????????????????????????????????????????\n",
    "# Fit the scaler to X_train and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the scaler to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# After fitting, you can transform the training and test data if needed\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.84417967e-02 -1.62823436e-01 -3.73283587e-01 -4.43387016e-02\n",
      "   1.25329068e-01 -1.75557108e-02 -4.94618697e-02 -2.58223628e-01\n",
      "   3.74143676e-01  7.84018383e-02  1.64169725e+00 -7.44333103e-02\n",
      "  -3.27398098e-01  1.51199468e+00 -1.90055678e-01  7.05973261e-01\n",
      "   1.15944336e-01 -3.45099898e-01  6.08468580e-03  7.83188746e+00\n",
      "   7.10708263e-01 -1.16300745e-01  8.23756701e-01  2.01038747e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -8.70397336e-03 -1.73306666e-01  5.29806142e-02\n",
      "   3.21641987e-01 -1.22745187e-01  5.10246830e-02  2.10678454e+00\n",
      "   2.08342875e+00]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "  -3.19924558e-01 -3.75563990e-01 -2.95841929e-01  1.17959774e+00\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -4.84613805e-01 -1.57775726e-01\n",
      "  -8.31630426e-03 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01  3.23710254e+00 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "   3.45899003e-01 -6.11845118e-02 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -1.02592159e-01 -2.13686862e-01\n",
      "  -4.13384669e-01]\n",
      " [-3.41222369e-01 -1.62823436e-01  4.23718633e+00 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "   6.66382932e-01 -3.75563990e-01 -2.95841929e-01  1.84081411e+00\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -7.77904856e-01 -1.57775726e-01\n",
      "   6.77649662e-01 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -1.68703137e-01  3.26772026e-01 -2.30922222e-01 -2.31071959e-01\n",
      "   3.08115098e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "   2.97545655e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01  6.50057568e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01  7.08530052e-01 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -8.66880815e-02 -1.94588332e-01\n",
      "  -1.97807006e-01]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "   1.48902533e+00 -3.44393902e-01  3.31412508e+00  2.96025836e+00\n",
      "   4.46549327e+00  1.67510923e+00 -2.95841929e-01 -6.21646859e-01\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "   2.68282502e+00 -3.45099898e-01 -1.91322753e-01  2.39501910e+00\n",
      "   2.59504851e+00 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -4.95023630e-01 -1.73306666e-01 -1.72933613e-01\n",
      "   1.67829135e-01 -1.22745187e-01  1.32846839e+00  6.36197727e-01\n",
      "   2.13047160e-02]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "  -3.19924558e-01 -3.75563990e-01 -2.95841929e-01  9.28791529e-01\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -1.63121690e-01 -1.57775726e-01\n",
      "   3.85953999e+00 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01  1.64238800e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -4.95023630e-01 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -8.70558058e-02 -2.13686862e-01\n",
      "  -4.39890119e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Scale the training data ?????????????????????????????????????????????????????????\n",
    "# Transform X_test using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Optionally, display the first few rows of the scaled training data\n",
    "print(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit a Logistic Regression Model\n",
    "\n",
    "Create a Logistic Regression model, fit it to the training data, make predictions with the testing data, and print the model's accuracy score. You may choose any starting settings you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9197\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model and print the model score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model using the scaled training data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Print the accuracy score of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted\n",
      "3683       0          0\n",
      "4412       0          0\n",
      "2584       0          0\n",
      "69         1          1\n",
      "1844       0          0\n"
     ]
    }
   ],
   "source": [
    "# Make and save testing predictions with the saved logistic regression model using the test data\n",
    "\n",
    "# Review the predictions\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved Logistic Regression model\n",
    "model = joblib.load('logistic_regression_model.pkl')\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to a DataFrame and save to a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df.to_csv('logistic_regression_predictions.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "print(predictions_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9197\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score by evaluating `y_test` vs. `testing_predictions`.????????????????\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X and y are already defined\n",
    "\n",
    "# Step 1: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 4: Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 5: Calculate and print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit a Random Forest Classifier Model\n",
    "\n",
    "Create a Random Forest Classifier model, fit it to the training data, make predictions with the testing data, and print the model's accuracy score. You may choose any starting settings you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 0.9555\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score?????????????\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Create the Random Forest Classifier instance\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 2: Train the model using the scaled training data\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 4: Calculate and print the accuracy score\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Classifier Accuracy: {accuracy_rf:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/logistic_regression_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the trained model to a file\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/data/logistic_regression_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load the saved Logistic Regression model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dev/lib/python3.10/site-packages/joblib/numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/logistic_regression_model.pkl'"
     ]
    }
   ],
   "source": [
    "# Make and save testing predictions with the saved logistic regression model using the test data\n",
    "\n",
    "\n",
    "# Review the predictions\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, '/mnt/data/logistic_regression_model.pkl')\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved Logistic Regression model\n",
    "model = joblib.load('/mnt/data/logistic_regression_model.pkl')\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to a DataFrame and save to a CSV file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df.to_csv('/mnt/data/logistic_regression_predictions.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "predictions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9548218940052129"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy score by evaluating `y_test` vs. `testing_predictions`.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X and y are already defined\n",
    "\n",
    "# Step 1: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 4: Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 5: Calculate and print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Models\n",
    "\n",
    "Which model performed better? How does that compare to your prediction? Write down your results and thoughts in the following markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace the text in this markdown cell with your answers to these questions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
